{"id":"BP0g","dependencies":[{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/package.json","includedInParent":true,"mtime":1609563696417},{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":499162500000},{"name":"@tensorflow/tfjs-core","loc":{"line":12,"column":77},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-core/dist/index.js"},{"name":"../backend/tfjs_backend","loc":{"line":13,"column":19},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/backend/tfjs_backend.js"},{"name":"../common","loc":{"line":14,"column":26},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/common.js"},{"name":"../errors","loc":{"line":15,"column":62},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/errors.js"},{"name":"../layers/serialization","loc":{"line":16,"column":28},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js"},{"name":"../losses","loc":{"line":17,"column":24},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/losses.js"},{"name":"../metrics","loc":{"line":18,"column":25},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/metrics.js"},{"name":"../optimizers","loc":{"line":19,"column":28},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/optimizers.js"},{"name":"../user_defined_metadata","loc":{"line":20,"column":41},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/user_defined_metadata.js"},{"name":"../utils/generic_utils","loc":{"line":21,"column":88},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js"},{"name":"../utils/layer_utils","loc":{"line":22,"column":29},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/layer_utils.js"},{"name":"../utils/math_utils","loc":{"line":23,"column":22},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/math_utils.js"},{"name":"../utils/serialization_utils","loc":{"line":24,"column":36},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/serialization_utils.js"},{"name":"../version","loc":{"line":25,"column":24},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/version.js"},{"name":"./container","loc":{"line":26,"column":26},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/container.js"},{"name":"./executor","loc":{"line":27,"column":34},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/executor.js"},{"name":"./training_dataset","loc":{"line":28,"column":44},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js"},{"name":"./training_tensors","loc":{"line":29,"column":138},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_tensors.js"},{"name":"./training_utils","loc":{"line":30,"column":81},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_utils.js"}],"generated":{"js":"\"use strict\";Object.defineProperty(exports,\"__esModule\",{value:!0}),exports.isDataTensor=T,exports.isDataArray=A,exports.isDataDict=z,exports.standardizeInputData=$,exports.checkArrayLengths=E,exports.collectMetrics=N,exports.Functional=exports.LayersModel=void 0;var e=O(require(\"@tensorflow/tfjs-core\")),t=O(require(\"../backend/tfjs_backend\")),s=require(\"../common\"),r=require(\"../errors\"),i=require(\"../layers/serialization\"),n=O(require(\"../losses\")),o=O(require(\"../metrics\")),a=O(require(\"../optimizers\")),l=require(\"../user_defined_metadata\"),h=require(\"../utils/generic_utils\"),u=require(\"../utils/layer_utils\"),c=require(\"../utils/math_utils\"),p=require(\"../utils/serialization_utils\"),f=require(\"../version\"),d=require(\"./container\"),g=require(\"./executor\"),m=require(\"./training_dataset\"),y=require(\"./training_tensors\"),w=require(\"./training_utils\");function b(){if(\"function\"!=typeof WeakMap)return null;var e=new WeakMap;return b=function(){return e},e}function O(e){if(e&&e.__esModule)return e;if(null===e||\"object\"!=typeof e&&\"function\"!=typeof e)return{default:e};var t=b();if(t&&t.has(e))return t.get(e);var s={},r=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var i in e)if(Object.prototype.hasOwnProperty.call(e,i)){var n=r?Object.getOwnPropertyDescriptor(e,i):null;n&&(n.get||n.set)?Object.defineProperty(s,i,n):s[i]=e[i]}return s.default=e,t&&t.set(e,s),s}function T(t){return t instanceof e.Tensor}function A(e){return Array.isArray(e)}function z(e){return!T(e)&&!A(e)}function $(e,t,s,i=!0,n=\"\"){if(null==t||0===t.length){if(null!=e){let t=!1;if(A(e)&&e.length>0)t=!0;else if(z(e)){for(const s in e)if(e.hasOwnProperty(s)){t=!0;break}}else t=!0;if(t)throw new r.ValueError(`Error when checking model ${n} expected no data, `+`but got ${e}`)}return[]}if(null==e)return t.map(e=>null);let o;if(z(e)){e=e,o=[];for(const s of t){if(null==e[s])throw new r.ValueError(`No data provided for \"${s}\". Need data for each key in: `+`${t}`);o.push(e[s])}}else if(A(e)){if((e=e).length!==t.length)throw new r.ValueError(`Error when checking model ${n}: the Array of `+\"Tensors that you are passing to your model is not the size the \"+`model expected. Expected to see ${t.length} Tensor(s), but `+`instead got the following list of Tensor(s): ${e}`);o=e}else{if(e=e,t.length>1)throw new r.ValueError(`The model ${n} expects ${t.length} Tensor(s), `+`but only received one Tensor. Found: Tensor with shape ${e.shape}`);o=[e]}if(o=(0,y.ensureTensorsRank2OrHigher)(o),null!=s)for(let a=0;a<t.length;++a){if(null==s[a])continue;const e=o[a];if(e.shape.length!==s[a].length)throw new r.ValueError(`Error when checking ${n}: expected ${t[a]} `+`to have ${s[a].length} dimension(s). but got array with `+`shape ${e.shape}`);for(let o=0;o<s[a].length;++o){if(0===o&&!i)continue;const l=e.shape[o],h=s[a][o];if(null!=h&&h>=0&&l!==h)throw new r.ValueError(`Error when checking ${n}: expected ${t[a]} `+`to have shape [${s[a]}], but got array with shape `+`[${e.shape}].`)}}return o}function E(t,s,i){const n=(0,h.unique)(t.map(e=>e.shape[0]));n.sort();const o=(0,h.unique)(s.map(e=>e.shape[0]));if(o.sort(),n.length>1)throw new r.ValueError(\"All input Tensors (x) should have the same number of samples. Got array shapes: \"+`${JSON.stringify(t.map(e=>e.shape))}`);if(o.length>1)throw new r.ValueError(\"All target Tensors (y) should have the same number of samples. Got array shapes: \"+`${JSON.stringify(s.map(e=>e.shape))}`);if(n.length>0&&o.length>0&&!e.util.arraysEqual(n,o))throw new r.ValueError(\"Input Tensors should have the same number of samples as target \"+`Tensors. Found ${n[0]} input sample(s) and ${o[0]} target `+\"sample(s).\")}function v(e,t,s){const i=[n.meanSquaredError,n.binaryCrossentropy,n.categoricalCrossentropy];for(let o=0;o<e.length;++o){const a=e[o],l=t[o],h=s[o];if(null!=l){if(l===n.categoricalCrossentropy&&1===a.shape[a.shape.length-1])throw new r.ValueError(`You are passing a target array of shape ${a.shape} while using `+\"a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].\");if(-1!==i.indexOf(l)){const e=a.shape.slice(1),t=h.slice(1);for(let s=0;s<e.length;++s){const i=e[s],n=t[s];if(null!=n&&i!==n)throw new r.ValueError(`A target Tensor with shape ${a.shape} was passed for an `+`output of shape ${h}, while using a loss function that `+\"expects targets to have the same shape as the output.\")}}}}}function x(e,t,s,i=!0,n=\"\"){let o;if(Array.isArray(e)){if(e.length!==t.length)throw new r.ValueError(`Error when checking model ${n}: the Array of `+\"Tensors that you are passing to your model is not the size the \"+`the model expected. Expected to see ${t.length} Tensor(s),`+` but instead got ${e.length} Tensors(s).`);o=e}else{if(t.length>1)throw new r.ValueError(`The model expects ${t.length} ${n} Tensors, `+\"but only received one Tensor. Found: array with shape \"+`${JSON.stringify(e.shape)}.`);o=[e]}if(null!=s)for(let a=0;a<t.length;++a){if(null==s[a])continue;const e=o[a];if(e.shape.length!==s[a].length)throw new r.ValueError(`Error when checking ${n}: expected ${t[a]} `+`to have ${s[a].length} dimension(s), but got array with `+`shape ${JSON.stringify(e.shape)}`);for(let o=0;o<s[a].length;++o){if(0===o&&!i)continue;const l=e.shape[o],h=s[a][o];if(null!=h&&h!==l)throw new r.ValueError(`Error when checking ${n}: expected `+`${t[a]} to have shape ${JSON.stringify(s[a])} but `+`got array with shape ${JSON.stringify(e.shape)}.`)}}}function N(e,t){if(null==e||Array.isArray(e)&&0===e.length)return t.map(e=>[]);let s;if(\"string\"==typeof e||\"function\"==typeof e)s=[e];else{if(!Array.isArray(e)&&\"object\"!=typeof e)throw new TypeError(\"Type of metrics argument not understood. Expected an string,\"+`function, Array, or Object, found: ${e}`);s=e}if(Array.isArray(s))return t.map(e=>s);{const e=[];for(const r of t){let t=s.hasOwnProperty(r)?s[r]:[];Array.isArray(t)||(t=[t]),e.push(t)}return e}}const k=\"layers-model\";class S extends d.Container{constructor(e){super(e),this.isTraining=!1}summary(e,t,s=console.log){if(!this.built)throw new r.ValueError(\"This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).\");(0,u.printSummary)(this,e,t,s)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,\"string\"==typeof t.optimizer)this.optimizer_=a.getOptimizer(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof e.Optimizer))throw new r.ValueError(\"User-defined optimizer must be an instance of tf.Optimizer.\");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let i=[];if(Array.isArray(t.loss)||\"string\"==typeof t.loss||\"function\"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new r.ValueError(\"When passing an Array as loss, it should have one entry per \"+`model output. The model has ${this.outputs.length} output(s), `+`but you passed loss=${t.loss}.`);const e=t.loss;i=e.map(e=>n.get(e))}else{const e=n.get(t.loss);this.outputs.forEach(t=>{i.push(e)})}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new r.ValueError(`Unknown entry in loss dictionary: \"${e}\". `+`Only expected the following keys: ${this.outputNames}`);for(const e of this.outputNames)null==t.loss[e]&&console.warn(`Output \"${e}\" is missing from loss dictionary. We assume `+\"this was done on purpose, and we will not be expecting data \"+`to be passed to ${e} during training`),i.push(n.get(t.loss[e]))}this.lossFunctions=i,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let e=0;e<this.outputs.length;++e){const t=this.internalOutputShapes[e],s=this.outputNames[e];this.feedOutputNames.push(s),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[e])}const l=[];this.metrics=t.metrics,this.metricsNames=[\"loss\"],this.metricsTensors=[],(0,s.nameScope)(\"loss\",()=>{for(let e=0;e<this.outputs.length;++e){if(-1!==l.indexOf(e))continue;const t=this.lossFunctions[e];this.outputs.length>1&&(this.metricsTensors.push([t,e]),this.metricsNames.push(this.outputNames[e]+\"_loss\"))}});const h=N(t.metrics,this.outputNames),u=(e,t,s)=>{this.outputNames.length>1&&(t=this.outputNames[e]+\"_\"+t),this.metricsNames.push(t),this.metricsTensors.push([s,e])};(0,s.nameScope)(\"metric\",()=>{for(let e=0;e<this.outputs.length;++e){if(-1!==l.indexOf(e))continue;(t=>{let r,i,a;for(const l of t){if(\"string\"==typeof l&&-1!==[\"accuracy\",\"acc\",\"crossentropy\",\"ce\"].indexOf(l)){const t=this.internalOutputShapes[e];let s;1===t[t.length-1]||this.lossFunctions[e]===n.binaryCrossentropy?-1!==[\"accuracy\",\"acc\"].indexOf(l)?i=o.binaryAccuracy:-1!==[\"crossentropy\",\"ce\"].indexOf(l)&&(i=o.binaryCrossentropy):this.lossFunctions[e]===n.sparseCategoricalCrossentropy?-1!==[\"accuracy\",\"acc\"].indexOf(l)?i=o.sparseCategoricalAccuracy:-1!==[\"crossentropy\",\"ce\"].indexOf(l)&&(i=o.sparseCategoricalCrossentropy):-1!==[\"accuracy\",\"acc\"].indexOf(l)?i=o.categoricalAccuracy:-1!==[\"crossentropy\",\"ce\"].indexOf(l)&&(i=o.categoricalCrossentropy),-1!==[\"accuracy\",\"acc\"].indexOf(l)?s=\"acc\":-1!==[\"crossentropy\",\"ce\"].indexOf(l)&&(s=\"ce\"),a=i,r=\"\"+s}else{const e=o.get(l);a=e,r=\"\"+o.getLossOrMetricName(l)}let t;(0,s.nameScope)(r,()=>{t=a}),u(e,r,t)}})(h[e])}}),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn(\"Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?\")}evaluate(e,t,s={}){const r=null==s.batchSize?32:s.batchSize;(0,y.checkBatchSize)(r);const i=this.standardizeUserDataXY(e,t,!0,r);try{const n=i[0].concat(i[1]);this.makeTestFunction();const o=this.testFunction,a=this.testLoop(o,n,r,s.verbose,s.steps);return(0,h.singletonOrArray)(a)}finally{(0,y.disposeNewTensors)(i[0],e),(0,y.disposeNewTensors)(i[1],t)}}async evaluateDataset(e,t){return this.makeTestFunction(),(0,m.evaluateDataset)(this,e,t)}checkNumSamples(e,t,s,i=\"steps\"){let n;if(null!=s){if(n=null,null!=t)throw new r.ValueError(`If ${i} is set, batchSize must be null or undefined.`+`Got batchSize = ${t}`)}else{if(null==e)throw new r.ValueError(\"Either the input data should have a defined shape, or \"+`${i} shoud be specified.`);n=Array.isArray(e)?e[0].shape[0]:e.shape[0]}return n}execute(t,s){if(Array.isArray(s)&&0===s.length)throw new r.ValueError(\"`outputs` is an empty Array, which is not allowed.\");const i=Array.isArray(s),n=i?s:[s],o=this.retrieveSymbolicTensors(n),a=new g.FeedDict;if(t instanceof e.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new r.ValueError(`The number of inputs provided (${t.length}) `+\"does not match the number of inputs of this model \"+`(${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const e of this.inputs){const s=t[e.name];if(null==s)throw new r.ValueError(`No value is provided for the model's input ${e.name}`);a.add(e,s)}const l=(0,g.execute)(o,a);return i?l:l[0]}retrieveSymbolicTensors(e){const t=(0,h.pyListRepeat)(null,e.length);let s=e.length;for(const r of this.layers){const i=Array.isArray(r.output)?r.output:[r.output],n=i.map(e=>e.name);for(let r=0;r<e.length;++r){const o=n.indexOf(e[r]);if(-1!==o&&(t[r]=i[o],s--),0===s)break}if(0===s)break}if(s>0){const s=[];throw t.forEach((t,r)=>{null==t&&s.push(e[r])}),new r.ValueError(\"Cannot find SymbolicTensors for output name(s): \"+`${JSON.stringify(s)}`)}return t}predictLoop(t,s=32,i=!1){return e.tidy(()=>{const n=this.checkNumSamples(t);if(i)throw new r.NotImplementedError(\"Verbose predictLoop() is not implemented yet.\");const o=(0,y.makeBatches)(n,s),a=this.outputs.map(e=>[]);for(let s=0;s<o.length;++s){e.tidy(()=>{const e=o[s][0],r=o[s][1],i=(0,y.sliceArrays)(t,e,r),n=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)n.push({key:this.inputs[t],value:i[t]});else n.push({key:this.inputs[0],value:i});const a=new g.FeedDict(n);return(0,g.execute)(this.outputs,a)}).forEach((e,t)=>a[t].push(e))}return(0,h.singletonOrArray)(a.map(t=>e.concat(t,0)))})}predict(e,t={}){const s=(0,y.ensureTensorsRank2OrHigher)(e);x(s,this.inputNames,this.feedInputShapes,!1);try{const r=null==t.batchSize?32:t.batchSize;return(0,y.checkBatchSize)(r),this.predictLoop(s,r)}finally{(0,y.disposeNewTensors)(s,e)}}predictOnBatch(e){x(e,this.inputNames,this.feedInputShapes,!0);const t=(Array.isArray(e)?e[0]:e).shape[0];return this.predictLoop(e,t)}standardizeUserDataXY(e,t,s=!0,i){if(null==this.optimizer_)throw new r.RuntimeError(\"You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).\");const o=[];for(let r=0;r<this.feedOutputShapes.length;++r){const e=this.feedOutputShapes[r];this.feedLossFns[r]===n.sparseCategoricalCrossentropy?o.push(e.slice(0,e.length-1).concat([1])):o.push(e)}if(E(e=$(e,this.feedInputNames,this.feedInputShapes,!1,\"input\"),t=$(t,this.feedOutputNames,o,!1,\"target\"),null),v(t,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&e[0].shape[0]%i!=0)throw new r.ValueError(\"In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size \"+`${i}. Found: ${e[0].shape[0]} sample(s).`);return[e,t]}async standardizeUserData(e,t,s,r,i=!0,n){const[o,a]=this.standardizeUserDataXY(e,t,i,n);if(null!=s)throw new Error(\"sample weight is not supported yet.\");let l=null;if(null!=r){const e=(0,w.standardizeClassWeights)(r,this.outputNames);l=[];for(let t=0;t<e.length;++t)l.push(await(0,w.standardizeWeights)(a[t],null,e[t]))}return[o,a,l]}testLoop(s,i,n,o=0,a){return e.tidy(()=>{const l=this.checkNumSamples(i,n,a,\"steps\"),h=[];if(o>0)throw new r.NotImplementedError(\"Verbose mode is not implemented yet.\");if(null!=a)throw new r.NotImplementedError(\"steps mode in testLoop() is not implemented yet\");{const r=(0,y.makeBatches)(l,n),o=(0,e.tensor1d)((0,c.range)(0,l));for(let n=0;n<r.length;++n){const a=r[n][0],l=r[n][1],u=t.sliceAlongFirstAxis(o,a,l-a),c=(0,y.sliceArraysByIndices)(i,u),p=s(c);if(0===n)for(let t=0;t<p.length;++t)h.push((0,e.scalar)(0));for(let t=0;t<p.length;++t){const s=p[t];h[t]=e.add(h[t],e.mul(l-a,s))}}for(let t=0;t<h.length;++t)h[t]=e.div(h[t],l)}return h})}getDedupedMetricsNames(){const e=this.metricsNames,t=[];for(let s=0;s<e.length;++s){const r=e[s];let i=r;if((0,h.count)(e,r)>1){i+=`_${(0,h.count)(e.slice(0,s),r)}`}t.push(i)}return t}makeTrainFunction(){return t=>{const s=[],r=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),n=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),o=[],a=this.collectedTrainableWeights.map(e=>e.read());return[this.optimizer_.minimize(()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:r[e]});const a=new g.FeedDict(t),l=(0,g.execute)(this.outputs,a,{training:!0});let h;for(let r=0;r<this.lossFunctions.length;++r){let t=(0,this.lossFunctions[r])(i[r],l[r]);null!=n[r]&&(t=(0,w.computeWeightedLoss)(t,n[r]));const o=e.mean(t);s.push(o),h=0===r?t:e.add(h,t)}for(let r=0;r<this.metricsTensors.length;++r){let t;if(this.outputs.length>1&&r<this.outputs.length)t=s[r];else{const s=this.metricsTensors[r][0],n=this.metricsTensors[r][1];t=e.mean(s(i[n],l[n]))}e.keep(t),o.push(t)}return h=e.mean(h),this.calculateLosses().forEach(t=>{h=e.add(h,t)}),h},!0,a)].concat(o)}}makeTestFunction(){this.testFunction=(t=>e.tidy(()=>{const s=[];let r;const i=t.slice(0,this.inputs.length),n=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),o=[];for(let e=0;e<this.inputs.length;++e)o.push({key:this.inputs[e],value:i[e]});const a=new g.FeedDict(o),l=(0,g.execute)(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],o=e.mean(i(n[t],l[t]));r=0===t?o:e.add(r,o),s.push(r)}for(let t=0;t<this.metricsTensors.length;++t){const r=this.metricsTensors[t][0],i=this.metricsTensors[t][1],o=e.mean(r(n[i],l[i]));s.push(o)}return s}))}async fit(e,t,s={}){return(0,y.fitTensors)(this,e,t,s)}async fitDataset(e,t){return(0,m.fitDataset)(this,e,t)}async trainOnBatch(t,s){const r=await this.standardizeUserData(t,s),i=r[0],n=r[1],o=this.makeTrainFunction()(i.concat(n)),a=[];for(const e of o){const t=await e.data();a.push(t[0])}return e.dispose(o),(0,h.singletonOrArray)(a)}getNamedWeights(e){const t=[],s=null!=e&&e.trainableOnly,r=s?this.trainableWeights:this.weights,i=this.getWeights(s);for(let n=0;n<r.length;++n)s&&!r[n].trainable||t.push({name:r[n].originalName,tensor:i[n]});return t}set stopTraining(e){this.stopTraining_=e}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(e){this.optimizer_!==e&&(this.optimizer_=e,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const s=e.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=s-e.memory().numTensors}return t}getLossIdentifiers(){let e;if(\"string\"==typeof this.loss)e=(0,h.toSnakeCase)(this.loss);else if(Array.isArray(this.loss)){for(const e of this.loss)if(\"string\"!=typeof e)throw new Error(\"Serialization of non-string loss is not supported.\");e=this.loss.map(e=>(0,h.toSnakeCase)(e))}else{const t=Object.keys(this.loss);e={};const s=this.loss;for(const r of t){if(\"string\"!=typeof s[r])throw new Error(\"Serialization of non-string loss is not supported.\");e[r]=(0,h.toSnakeCase)(s[r])}}return e}getMetricIdentifiers(){if(\"string\"==typeof this.metrics||\"function\"==typeof this.metrics)return[(0,h.toSnakeCase)(o.getLossOrMetricName(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map(e=>(0,h.toSnakeCase)(o.getLossOrMetricName(e)));{const e={};for(const t in this.metrics)e[t]=(0,h.toSnakeCase)(o.getLossOrMetricName(this.metrics[t]));return e}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(e){if(null!=e.weighted_metrics)throw new Error(\"Loading weight_metrics is not supported yet.\");if(null!=e.loss_weights)throw new Error(\"Loading loss_weights is not supported yet.\");if(null!=e.sample_weight_mode)throw new Error(\"Loading sample_weight_mode is not supported yet.\");const t=(0,p.convertPythonicToTs)(e.optimizer_config),s=(0,i.deserialize)(t);let r,n;if(\"string\"==typeof e.loss)r=(0,h.toCamelCase)(e.loss);else if(Array.isArray(e.loss))r=e.loss.map(e=>(0,h.toCamelCase)(e));else if(null!=e.loss){r={};for(const t in e.loss)r[t]=(0,h.toCamelCase)(e.loss[t])}if(Array.isArray(e.metrics))n=e.metrics.map(e=>(0,h.toCamelCase)(e));else if(null!=e.metrics){n={};for(const t in e.metrics)n[t]=(0,h.toCamelCase)(e.metrics[t])}this.compile({loss:r,metrics:n,optimizer:s})}async save(t,s){if(\"string\"==typeof t){const s=e.io.getSaveHandlers(t);if(0===s.length)throw new r.ValueError(`Cannot find any save handlers for URL '${t}'`);if(s.length>1)throw new r.ValueError(`Found more than one (${s.length}) save handlers for `+`URL '${t}'`);t=s[0]}if(null==t.save)throw new r.ValueError(\"LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");const i=await e.io.encodeWeights(this.getNamedWeights(s)),n={modelTopology:this.toJSON(null,!1),format:k,generatedBy:`TensorFlow.js tfjs-layers v${f.version}`,convertedBy:null};if(null!=s&&s.includeOptimizer&&null!=this.optimizer){n.trainingConfig=this.getTrainingConfig();const t=\"optimizer\",{data:s,specs:r}=await e.io.encodeWeights(await this.optimizer.getWeights(),t);i.specs.push(...r),i.data=e.io.concatenateArrayBuffers([i.data,s])}if(null!=this.userDefinedMetadata){const e=!0;(0,l.checkUserDefinedMetadata)(this.userDefinedMetadata,this.name,e),n.userDefinedMetadata=this.userDefinedMetadata}return n.weightData=i.data,n.weightSpecs=i.specs,t.save(n)}setUserDefinedMetadata(e){(0,l.checkUserDefinedMetadata)(e,this.name),this.userDefinedMetadata=e}getUserDefinedMetadata(){return this.userDefinedMetadata}}exports.LayersModel=S,S.className=\"Model\",e.serialization.registerClass(S);class C extends S{}exports.Functional=C,C.className=\"Functional\",e.serialization.registerClass(C);"},"sourceMaps":null,"error":null,"hash":"75ceaa5a16496490c459773dd9c1842f","cacheData":{"env":{}}}