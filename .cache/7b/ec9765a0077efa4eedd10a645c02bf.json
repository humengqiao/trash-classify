{"id":"WFf5","dependencies":[{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/package.json","includedInParent":true,"mtime":1609563696417},{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":499162500000},{"name":"@tensorflow/tfjs-core","loc":{"line":14,"column":23},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-core/dist/index.js"},{"name":"../base_callbacks","loc":{"line":15,"column":57},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/base_callbacks.js"},{"name":"../errors","loc":{"line":16,"column":48},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/errors.js"},{"name":"../logs","loc":{"line":17,"column":37},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/logs.js"},{"name":"../utils/generic_utils","loc":{"line":18,"column":41},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js"},{"name":"./training_utils","loc":{"line":19,"column":60},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/training_utils.js"}],"generated":{"js":"\"use strict\";Object.defineProperty(exports,\"__esModule\",{value:!0}),exports.fitDataset=p,exports.evaluateDataset=b;var t=r(require(\"@tensorflow/tfjs-core\")),e=require(\"../base_callbacks\"),a=require(\"../errors\"),s=require(\"../logs\"),i=require(\"../utils/generic_utils\"),n=require(\"./training_utils\");function o(){if(\"function\"!=typeof WeakMap)return null;var t=new WeakMap;return o=function(){return t},t}function r(t){if(t&&t.__esModule)return t;if(null===t||\"object\"!=typeof t&&\"function\"!=typeof t)return{default:t};var e=o();if(e&&e.has(t))return e.get(t);var a={},s=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var i in t)if(Object.prototype.hasOwnProperty.call(t,i)){var n=s?Object.getOwnPropertyDescriptor(t,i):null;n&&(n.get||n.set)?Object.defineProperty(a,i,n):a[i]=t[i]}return a.default=t,e&&e.set(t,a),a}const l=32;function c(e,a){let s,i;const n=a;s=n.xs,i=n.ys,t.util.assert(null!=s&&null!=i,()=>\"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates \"+`${a}`);const o=u(\"input\",e.inputNames,s),r=u(\"output\",e.outputNames,i),l=o[0].shape[0];t.util.assert(o.length===e.inputs.length,()=>`LayersModel has ${e.inputs.length} inputs, but the dataset `+`provides ${o.length} inputs.  (Expected input keys: `+`${JSON.stringify(e.inputNames)})`),t.util.assert(r.length===e.outputs.length,()=>`LayersModel has ${e.outputs.length} outputs, but the dataset `+`provides ${r.length} outputs.  (Expected output keys: `+`${JSON.stringify(e.outputNames)})`);for(let c=0;c<o.length;c++)t.util.assert(o[c].shape[0]===l,()=>\"Batch size mismatch: input \"+`${e.inputNames[c]} has ${o[c].shape[0]}; `+`expected  ${l} based on input ${e.inputNames[0]}.`);for(let c=0;c<r.length;c++)t.util.assert(r[c].shape[0]===l,()=>\"Batch size mismatch: output \"+`${e.outputNames[c]} has ${r[c].shape[0]}; `+`expected  ${l} based on input ${e.inputNames[0]}.`);return{xs:o,ys:r}}function u(e,s,i){if(i instanceof t.Tensor)return[i];if(Array.isArray(i))return t.util.assert(i.length===s.length,()=>`Received an array of ${i.length} Tensors, but expected ${s.length} to match the ${e} keys ${s}.`),i;{const t=[];for(const n of s){if(null==i[n])throw new a.ValueError(\"The feature data generated by the dataset lacks the required \"+`${e} key '${n}'.`);t.push(i[n])}return t}}function h(t){if(3===t.length)throw new a.NotImplementedError(\"Validation with sample weights is not implemented yet.\");return{xs:t[0],ys:t[1]}}async function p(a,o,r){const u=null!=r.batchesPerEpoch;if(t.util.assert(null!=a.optimizer,()=>\"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).\"),t.util.assert(null!=r,()=>\"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.\"),t.util.assert(null!=r.epochs&&r.epochs>0&&Number.isInteger(r.epochs),()=>\"For fitDataset(), config.epochs is expected to be a positive \"+`integer, but got ${r.epochs}`),t.util.assert(!u||r.batchesPerEpoch>0&&Number.isInteger(r.batchesPerEpoch),()=>\"For fitDataset(), config.batchesPerEpoch is expected to be a \"+`positive integer if specified, but got ${r.batchesPerEpoch}`),t.util.assert(null==r.validationSplit,()=>\"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.\"),a.isTraining)throw new Error(\"Cannot start training because another fit() call is ongoing.\");a.isTraining=!0;try{const p=null!=r.validationData;let g,b;if(p)if(f(r.validationData))t.util.assert(null==r.validationBatches||r.validationBatches>0&&Number.isInteger(r.validationBatches),()=>\"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, \"+`but got ${r.validationBatches}`);else{const t=h(r.validationData);g=t.xs,b=t.ys}const y=a.makeTrainFunction(),v=a.getDedupedMetricsNames();let m;m=p?v.slice().concat(v.map(t=>\"val_\"+t)):v.slice();const w=(0,e.standardizeCallbacks)(r.callbacks,r.yieldEvery),$=null==r.verbose?1:r.verbose,{callbackList:E,history:x}=(0,e.configureCallbacks)(w,$,r.epochs,null,null,d(o,r),null,p,m);E.setModel(a),a.history=x,await E.onTrainBegin(),a.stopTraining_=!1;let D=null==r.initialEpoch?0:r.initialEpoch,N=await o.iterator();for(;D<r.epochs;){const e={};await E.onEpochBegin(D);let h=0,d=0;for(u||(N=await o.iterator());!u||h<r.batchesPerEpoch;){const o=await N.next();if(u&&o.done){console.warn(\"You provided `batchesPerEpoch` as \"+`${r.batchesPerEpoch}, `+\"but your dataset iterator ran out of data after \"+`${h} batches; `+\"interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, \"+`${r.batchesPerEpoch*r.epochs} batches). `+\"You may need to use the repeat() function when building your dataset.\");break}if(null!=o.value){const{xs:e,ys:i}=c(a,o.value),l={};l.batch=d,l.size=e[0].shape[0],await E.onBatchBegin(d,l);const u=[];if(null!=r.classWeight){const t=(0,n.standardizeClassWeights)(r.classWeight,a.outputNames);for(let e=0;e<t.length;++e)u.push(await(0,n.standardizeWeights)(i[e],null,t[e]))}const p=e.concat(i).concat(u),f=y(p);t.dispose(p);for(let a=0;a<v.length;++a){const e=v[a],s=f[a];l[e]=s,t.keep(s)}await E.onBatchEnd(d,l),(0,s.disposeTensorsInLogs)(l),d++,h++}if(u?h>=r.batchesPerEpoch:o.done){if(p){let t;t=f(r.validationData)?(0,i.toList)(await a.evaluateDataset(r.validationData,{batches:r.validationBatches})):(0,i.toList)(a.evaluate(g,b,{batchSize:null==r.validationBatchSize?l:r.validationBatchSize,verbose:0}));for(let s=0;s<a.metricsNames.length;++s)e[`val_${a.metricsNames[s]}`]=t[s]}break}if(a.stopTraining_)break}if(await E.onEpochEnd(D,e),D++,a.stopTraining_)break}return await E.onTrainEnd(),await a.history.syncData(),a.history}finally{a.isTraining=!1}}function d(t,e){let a=null;return null!=e.batchesPerEpoch?a=e.batchesPerEpoch:Number.isFinite(t.size)&&(a=t.size),a}function f(t){return\"function\"==typeof t.iterator}function g(t){return\"function\"==typeof t.next}async function b(e,s,n){const o=null!=(n=n||{}).batches,r=e.testFunction;let l=[];if(n.verbose>0)throw new a.NotImplementedError(\"Verbose mode is not implemented yet.\");t.util.assert(!o||n.batches>0&&Number.isInteger(n.batches),()=>\"Test loop expects `batches` to be a positive integer, but \"+`received ${JSON.stringify(n.batches)}`);const u=g(s)?s:await s.iterator();let h=0,p=0;for(;!o||p<n.batches;){const a=await u.next();if(l=t.tidy(()=>{if(a.value){const{xs:s,ys:i}=c(e,a.value),n=s.concat(i),o=t.tidy(()=>r(n));if(t.dispose(n),0===p)for(let e=0;e<o.length;++e)l.push((0,t.scalar)(0));const u=n[0].shape[0];for(let e=0;e<o.length;++e){const a=o[e],s=l[e];l[e]=t.tidy(()=>t.add(l[e],t.mul(u,a))),p>0&&t.dispose(s)}t.dispose(o),h+=u,++p}return l}),a.done){o&&console.warn(\"Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least `batches` \"+`batches (in this case, ${n.batches} batches). `+\"You may need to use the repeat() function when building your dataset.\");break}}for(let a=0;a<l.length;++a){const e=l[a];l[a]=t.div(l[a],h),t.dispose(e)}return(0,i.singletonOrArray)(l)}"},"sourceMaps":null,"error":null,"hash":"2780be19b4df7f8cab04a7cb1e8bc6ec","cacheData":{"env":{}}}