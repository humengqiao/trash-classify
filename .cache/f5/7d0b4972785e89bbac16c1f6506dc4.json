{"id":"fh9C","dependencies":[{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/package.json","includedInParent":true,"mtime":1609565653941},{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/package.json","includedInParent":true,"mtime":499162500000},{"name":"@tensorflow/tfjs-core","loc":{"line":18,"column":34},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/iterators/microphone_iterator.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-core/dist/index.js"},{"name":"./lazy_iterator","loc":{"line":19,"column":29},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/iterators/microphone_iterator.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/iterators/lazy_iterator.js"}],"generated":{"js":"\"use strict\";Object.defineProperty(exports,\"__esModule\",{value:!0}),exports.MicrophoneIterator=void 0;var t=require(\"@tensorflow/tfjs-core\"),e=require(\"./lazy_iterator\");class a extends e.LazyIterator{constructor(t){super(),this.microphoneConfig=t,this.isClosed=!1,this.fftSize=t.fftSize||1024;const e=Math.log2(this.fftSize);if(this.fftSize<0||e<4||e>14||!Number.isInteger(e))throw new Error(\"Invalid fftSize: it must be a power of 2 between \"+`2 to 4 and 2 to 14, but got ${this.fftSize}`);if(this.numFrames=t.numFramesPerSpectrogram||43,this.sampleRateHz=t.sampleRateHz,this.columnTruncateLength=t.columnTruncateLength||this.fftSize,this.audioTrackConstraints=t.audioTrackConstraints,this.smoothingTimeConstant=t.smoothingTimeConstant||0,this.includeSpectrogram=!1!==t.includeSpectrogram,this.includeWaveform=!0===t.includeWaveform,!this.includeSpectrogram&&!this.includeWaveform)throw new Error(\"Both includeSpectrogram and includeWaveform are false. At least one type of data should be returned.\")}summary(){return\"microphone\"}static async create(e={}){if((0,t.env)().get(\"IS_NODE\"))throw new Error(\"microphone API is only supported in browser environment.\");const i=new a(e);return await i.start(),i}async start(){try{this.stream=await navigator.mediaDevices.getUserMedia({audio:null==this.audioTrackConstraints||this.audioTrackConstraints,video:!1})}catch(a){throw new Error(`Error thrown while initializing video stream: ${a.message}`)}if(!this.stream)throw new Error(\"Could not obtain audio from microphone.\");const t=window.AudioContext||window.webkitAudioContext;if(this.audioContext=new t,this.sampleRateHz){if(this.audioContext.sampleRate!==this.sampleRateHz)throw new Error(\"Mismatch in sampling rate: \"+`Expected: ${this.sampleRateHz}; `+`Actual: ${this.audioContext.sampleRate}`)}else this.sampleRateHz=this.audioContext.sampleRate;const e=this.audioContext.createMediaStreamSource(this.stream);this.analyser=this.audioContext.createAnalyser(),this.analyser.fftSize=2*this.fftSize,this.analyser.smoothingTimeConstant=this.smoothingTimeConstant,e.connect(this.analyser),this.freqData=new Float32Array(this.fftSize),this.timeData=new Float32Array(this.fftSize)}async next(){if(this.isClosed)return{value:null,done:!0};let t,e;const a=await this.getAudioData();if(this.includeSpectrogram){const e=this.flattenQueue(a.freqDataQueue);t=this.getTensorFromAudioDataArray(e,[this.numFrames,this.columnTruncateLength,1])}if(this.includeWaveform){const t=this.flattenQueue(a.timeDataQueue);e=this.getTensorFromAudioDataArray(t,[this.numFrames*this.fftSize,1])}return{value:{spectrogram:t,waveform:e},done:!1}}async capture(){return(await this.next()).value}async getAudioData(){const t=[],e=[];let a=0;return new Promise(i=>{const r=setInterval(()=>{this.includeSpectrogram&&(this.analyser.getFloatFrequencyData(this.freqData),this.freqData[0]===-1/0&&i({freqDataQueue:t,timeDataQueue:e}),t.push(this.freqData.slice(0,this.columnTruncateLength))),this.includeWaveform&&(this.analyser.getFloatTimeDomainData(this.timeData),e.push(this.timeData.slice())),++a===this.numFrames&&(clearInterval(r),i({freqDataQueue:t,timeDataQueue:e}))},this.fftSize/this.sampleRateHz*1e3)})}stop(){this.isClosed||(this.isClosed=!0,this.analyser.disconnect(),this.audioContext.close(),null!=this.stream&&this.stream.getTracks().length>0&&this.stream.getTracks()[0].stop())}toArray(){throw new Error(\"Can not convert infinite audio stream to array.\")}getSampleRate(){return this.sampleRateHz}flattenQueue(t){const e=t[0].length,a=new Float32Array(t.length*e);return t.forEach((t,i)=>a.set(t,i*e)),a}getTensorFromAudioDataArray(e,a){const i=new Float32Array(t.util.sizeFromShape(a));return i.set(e,i.length-e.length),(0,t.tensor)(i,a)}}exports.MicrophoneIterator=a;"},"sourceMaps":null,"error":null,"hash":"9108fc4230d3b26950059eb29a1dce0f","cacheData":{"env":{}}}