{"id":"wLsA","dependencies":[{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/package.json","includedInParent":true,"mtime":1609563696417},{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/package.json","includedInParent":true,"mtime":499162500000},{"name":"@tensorflow/tfjs-core","loc":{"line":18,"column":20},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-core/dist/index.js"},{"name":"seedrandom","loc":{"line":19,"column":28},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/seedrandom/index.js"},{"name":"./iterators/lazy_iterator","loc":{"line":20,"column":119},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/iterators/lazy_iterator.js"},{"name":"./util/deep_map","loc":{"line":21,"column":61},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/dataset.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-data/dist/util/deep_map.js"}],"generated":{"js":"\"use strict\";Object.defineProperty(exports,\"__esModule\",{value:!0}),exports.datasetFromIteratorFn=o,exports.array=u,exports.zip=l,exports.Dataset=void 0;var t=n(require(\"@tensorflow/tfjs-core\")),e=n(require(\"seedrandom\")),r=require(\"./iterators/lazy_iterator\"),i=require(\"./util/deep_map\");function s(){if(\"function\"!=typeof WeakMap)return null;var t=new WeakMap;return s=function(){return t},t}function n(t){if(t&&t.__esModule)return t;if(null===t||\"object\"!=typeof t&&\"function\"!=typeof t)return{default:t};var e=s();if(e&&e.has(t))return e.get(t);var r={},i=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var n in t)if(Object.prototype.hasOwnProperty.call(t,n)){var a=i?Object.getOwnPropertyDescriptor(t,n):null;a&&(a.get||a.set)?Object.defineProperty(r,n,a):r[n]=t[n]}return r.default=t,e&&e.set(t,r),r}class a{constructor(){this.size=null}batch(e,r=!0){const i=this;let s;return t.util.assert(e>0,()=>`batchSize needs to be positive, but it is\\n      ${e}`),o(async()=>(await i.iterator()).columnMajorBatch(e,r,c),s=this.size===1/0||null==this.size?this.size:r?Math.ceil(this.size/e):Math.floor(this.size/e))}concatenate(t){const e=this;let r;return o(async()=>(await e.iterator()).concatenate(await t.iterator()),r=this.size===1/0||t.size===1/0?1/0:null!=this.size&&null!=t.size?this.size+t.size:null)}filter(e){const r=this;let i;return o(async()=>(await r.iterator()).filter(r=>t.tidy(()=>e(r))),i=this.size===1/0?1/0:null)}async forEachAsync(t){return(await this.iterator()).forEachAsync(t)}map(e){const r=this;return o(async()=>(await r.iterator()).map(r=>t.tidy(()=>e(r))),this.size)}mapAsync(t){const e=this;return o(async()=>(await e.iterator()).mapAsync(t),this.size)}prefetch(t){if(null==t)throw new RangeError(\"`Dataset.prefetch()` requires bufferSize to be specified.\");const e=this;return o(async()=>(await e.iterator()).prefetch(t),this.size)}repeat(t){const e=this;let i;return o(async()=>{const i=(0,r.iteratorFromFunction)(async()=>({value:await e.iterator(),done:!1}));return(0,r.iteratorFromConcatenated)(i.take(t))},i=null!=this.size&&t>0?this.size*t:0===t?0:null!=this.size&&(void 0===t||t<0)?1/0:null)}skip(t){const e=this;let r;return o(async()=>(await e.iterator()).skip(t),r=null!=this.size&&t>=0&&this.size>=t?this.size-t:null!=this.size&&(this.size<t||void 0===t||t<0)?0:null)}shuffle(r,i,s=!0){if(null==r||r<0)throw null==this.size?new RangeError(\"`Dataset.shuffle()` requires bufferSize to be specified.\"):new RangeError(\"`Dataset.shuffle()` requires bufferSize to be specified.  If your data fits in main memory (for regular JS objects), and/or GPU memory (for `tf.Tensor`s), consider setting \"+`bufferSize to the dataset size (${this.size} elements)`);const n=this,a=e.alea(i||t.util.now().toString());return o(async()=>{let t=a.int32();return s&&(t+=a.int32()),(await n.iterator()).shuffle(r,t.toString())},this.size)}take(t){const e=this;let r;return o(async()=>(await e.iterator()).take(t),r=null!=this.size&&this.size>t?t:null!=this.size&&this.size<=t?this.size:null)}async toArray(){if(this.size===1/0)throw new Error(\"Can not convert infinite data stream to array.\");return(await this.iterator()).toArray()}async toArrayForTest(){if(this.size===1/0)throw new Error(\"Can not convert infinite data stream to array.\");return(await this.iterator()).toArrayForTest()}}function o(t,e=null){return new class extends a{constructor(){super(...arguments),this.size=e}async iterator(){return t()}}}function u(t){return o(async()=>(0,r.iteratorFromItems)(t),t.length)}function l(t){if(!(0,i.isIterable)(t))throw new Error(\"The argument to zip() must be an object or array.\");let e;if(Array.isArray(t))for(let r=0;r<t.length;r++)e=null==e?t[r].size:Math.min(e,t[r].size);else if(t instanceof Object)for(const r in t)e=null==e?t[r].size:Math.min(e,t[r].size);return o(async()=>{const e=await(0,i.deepMapAndAwaitAll)(t,t=>{if(t instanceof a)return{value:t.iterator(),recurse:!1};if((0,i.isIterable)(t))return{value:null,recurse:!0};throw new Error(\"Leaves of the structure passed to zip() must be Datasets, not primitives.\")});return(0,r.iteratorFromZipped)(e,r.ZipMismatchMode.SHORTEST)},e)}function c(t){if(null===t)return null;const e=t[0];if((0,i.canTensorify)(e)){return{value:f(t),recurse:!1}}return{value:null,recurse:!0}}function f(e){if(0===e.length)throw new Error(\"Can't make a batch of zero elements.\");return e[0]instanceof t.Tensor?t.stack(e):t.tensor(e)}exports.Dataset=a,a.MAX_BUFFER_SIZE=1e4;"},"sourceMaps":null,"error":null,"hash":"1f454e89e350fedc4617b5128d4d5db4","cacheData":{"env":{}}}