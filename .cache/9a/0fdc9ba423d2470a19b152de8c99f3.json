{"id":"SdIK","dependencies":[{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/package.json","includedInParent":true,"mtime":1609563696417},{"name":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":499162500000},{"name":"@tensorflow/tfjs-core","loc":{"line":14,"column":42},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-core/dist/index.js"},{"name":"../activations","loc":{"line":15,"column":51},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/activations.js"},{"name":"../backend/tfjs_backend","loc":{"line":16,"column":19},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/backend/tfjs_backend.js"},{"name":"../common","loc":{"line":17,"column":26},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/common.js"},{"name":"../constraints","loc":{"line":18,"column":51},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/constraints.js"},{"name":"../engine/topology","loc":{"line":20,"column":22},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/engine/topology.js"},{"name":"../errors","loc":{"line":21,"column":64},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/errors.js"},{"name":"../initializers","loc":{"line":22,"column":72},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/initializers.js"},{"name":"../regularizers","loc":{"line":23,"column":53},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/regularizers.js"},{"name":"../utils/generic_utils","loc":{"line":24,"column":38},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/generic_utils.js"},{"name":"../utils/math_utils","loc":{"line":25,"column":28},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/math_utils.js"},{"name":"../utils/types_utils","loc":{"line":26,"column":73},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/utils/types_utils.js"},{"name":"../variables","loc":{"line":27,"column":45},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/variables.js"},{"name":"./serialization","loc":{"line":28,"column":28},"parent":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/recurrent.js","resolved":"/Users/humengqiao/Desktop/node-project/trash-classify/node_modules/@tensorflow/tfjs-layers/dist/layers/serialization.js"}],"generated":{"js":"\"use strict\";Object.defineProperty(exports,\"__esModule\",{value:!0}),exports.standardizeArgs=I,exports.rnn=R,exports.generateDropoutMask=E,exports.StackedRNNCells=exports.LSTM=exports.LSTMCell=exports.GRU=exports.GRUCell=exports.SimpleRNN=exports.SimpleRNNCell=exports.RNNCell=exports.RNN=void 0;var t=d(require(\"@tensorflow/tfjs-core\")),e=require(\"../activations\"),i=d(require(\"../backend/tfjs_backend\")),r=require(\"../common\"),s=require(\"../constraints\"),n=require(\"../engine/topology\"),a=require(\"../errors\"),l=require(\"../initializers\"),u=require(\"../regularizers\"),o=require(\"../utils/generic_utils\"),h=d(require(\"../utils/math_utils\")),c=require(\"../utils/types_utils\"),p=require(\"../variables\"),g=require(\"./serialization\");function z(){if(\"function\"!=typeof WeakMap)return null;var t=new WeakMap;return z=function(){return t},t}function d(t){if(t&&t.__esModule)return t;if(null===t||\"object\"!=typeof t&&\"function\"!=typeof t)return{default:t};var e=z();if(e&&e.has(t))return e.get(t);var i={},r=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var s in t)if(Object.prototype.hasOwnProperty.call(t,s)){var n=r?Object.getOwnPropertyDescriptor(t,s):null;n&&(n.get||n.set)?Object.defineProperty(i,s,n):i[s]=t[s]}return i.default=t,e&&e.set(t,i),i}function I(t,e,i,r){if(Array.isArray(t)){if(null!=e||null!=i)throw new a.ValueError(\"When inputs is an array, neither initialState or constants should be provided\");null!=r&&(i=t.slice(t.length-r,t.length),t=t.slice(0,t.length-r)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function s(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=s(e),constants:i=s(i)}}function R(e,i,r,s=!1,n,l,u=!1,o=!1){return t.tidy(()=>{const c=i.shape.length;if(c<3)throw new a.ValueError(`Input should be at least 3D, but is ${c}D.`);const p=[1,0].concat(h.range(2,c));if(i=t.transpose(i,p),null!=l)throw new a.NotImplementedError(\"The rnn() functoin of the deeplearn.js backend does not support constants yet.\");u&&console.warn(\"Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.\"),null!=n&&((n=n.asType(\"bool\").asType(\"float32\")).rank===c-1&&(n=t.expandDims(n,-1)),n=t.transpose(n,p)),s&&(i=t.reverse(i,0),null!=n&&(n=t.reverse(n,0)));const g=[];let z,d=r;const I=i.shape[0],R=t.unstack(i);let k,m;null!=n&&(k=t.unstack(n));for(let i=0;i<I;++i){const r=R[i],s=t.tidy(()=>e(r,d));if(null==n)z=s[0],d=s[1];else{const e=t.tidy(()=>{const e=k[i],r=t.onesLike(e).sub(e);return{output:s[0].mul(e).add(d[0].mul(r)),newStates:d.map((t,i)=>s[1][i].mul(e).add(t.mul(r)))}});z=e.output,d=e.newStates}o&&g.push(z)}if(o){const e=1;m=t.stack(g,e)}return[z,m,d]})}class k extends n.Layer{constructor(t){let e;if(super(t),null==t.cell)throw new a.ValueError(\"cell property is missing for the constructor of RNN.\");if(null==(e=Array.isArray(t.cell)?new N({cells:t.cell}):t.cell).stateSize)throw new a.ValueError(\"The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).\");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new n.InputSpec({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return h.range(0,t).map(t=>null)}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){(0,c.isArrayOfShapes)(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const i=e[0];let r;if(r=this.returnSequences?[t[0],t[1],i]:[t[0],i],this.returnState){const i=[];for(const r of e)i.push([t[0],r]);return[r].concat(i)}return r}computeMask(e,i){return t.tidy(()=>{Array.isArray(i)&&(i=i[0]);const t=this.returnSequences?i:null;if(this.returnState){const e=this.states.map(t=>null);return[t].concat(e)}return t})}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let i=0;i<t;++i)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(e){if(null!=this.numConstants)throw new a.NotImplementedError(\"Constants support is not implemented in RNN yet.\");(0,c.isArrayOfShapes)(e)&&(e=e[0]),e=e;const i=this.stateful?e[0]:null,r=e.slice(2);this.inputSpec[0]=new n.InputSpec({shape:[i,null,...r]});const s=[e[0]].concat(e.slice(2));let l;if(this.cell.build(s),l=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!t.util.arraysEqual(this.stateSpec.map(t=>t.shape[t.shape.length-1]),l))throw new a.ValueError(\"An initialState was passed that is not compatible with \"+`cell.stateSize. Received stateSpec=${this.stateSpec}; `+`However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=l.map(t=>new n.InputSpec({shape:[null,t]}));this.stateful&&this.resetStates()}resetStates(e,i=!1){(0,t.tidy)(()=>{if(!this.stateful)throw new a.AttributeError(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");const r=this.inputSpec[0].shape[0];if(null==r)throw new a.ValueError(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(e=>t.zeros([r,e])):this.states_=[t.zeros([r,this.cell.stateSize])];else if(null==e)t.dispose(this.states_),null!=this.keptStates&&(t.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(e=>t.zeros([r,e])):this.states_[0]=t.zeros([r,this.cell.stateSize]);else{if(Array.isArray(e)||(e=[e]),e.length!==this.states_.length)throw new a.ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), `+`but it received ${e.length} state value(s). Input `+`received: ${e}`);!0===i?this.keptStates.push(this.states_.slice()):t.dispose(this.states_);for(let i=0;i<this.states_.length;++i){const s=e[i],n=Array.isArray(this.cell.stateSize)?this.cell.stateSize[i]:this.cell.stateSize,l=[r,n];if(!t.util.arraysEqual(s.shape,l))throw new a.ValueError(`State ${i} is incompatible with layer ${this.name}: `+`expected shape=${l}, received shape=${s.shape}`);this.states_[i]=s}}this.states_=this.states_.map(e=>t.keep(e.clone()))})}apply(t,e){let i=null==e?null:e.initialState,r=null==e?null:e.constants;null==e&&(e={});const s=I(t,i,r,this.numConstants);t=s.inputs,i=s.initialState,r=s.constants;let a=[],l=[];if(null!=i){e.initialState=i,a=a.concat(i),this.stateSpec=[];for(const t of i)this.stateSpec.push(new n.InputSpec({shape:t.shape}));l=l.concat(this.stateSpec)}if(null!=r&&(e.constants=r,a=a.concat(r),this.numConstants=r.length),a[0]instanceof n.SymbolicTensor){const i=[t].concat(a),r=this.inputSpec.concat(l),s=this.inputSpec;this.inputSpec=r;const n=super.apply(i,e);return this.inputSpec=s,n}return super.apply(t,e)}call(e,i){return(0,t.tidy)(()=>{const t=null==i?null:i.mask,r=null==i?null:i.training;let s=null==i?null:i.initialState;e=(0,c.getExactlyOneTensor)(e),null==s&&(s=this.stateful?this.states_:this.getInitialState(e));const n=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(s.length!==n)throw new a.ValueError(`RNN Layer has ${n} state(s) but was passed `+`${s.length} initial state(s).`);this.unroll&&console.warn(\"Ignoring unroll = true for RNN layer, due to imperative backend.\");const l={training:r},u=R((t,e)=>{const i=this.cell.call([t].concat(e),l);return[i[0],i.slice(1)]},e,s,this.goBackwards,t,null,this.unroll,this.returnSequences),o=u[0],h=u[1],p=u[2];this.stateful&&this.resetStates(p,r);const g=this.returnSequences?h:o;return this.returnState?[g].concat(p):g})}getInitialState(e){return(0,t.tidy)(()=>{let r=t.zeros(e.shape);return r=t.sum(r,[1,2]),r=i.expandDims(r),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(t=>t>1?i.tile(r,[1,t]):r):this.cell.stateSize>1?[i.tile(r,[1,this.cell.stateSize])]:[r]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const i=this.cell.getConfig();return this.getClassName()===k.className&&(e.cell={className:this.cell.getClassName(),config:i}),Object.assign({},i,t,e)}static fromConfig(t,e,i={}){const r=e.cell,s=(0,g.deserialize)(r,i);return new t(Object.assign(e,{cell:s}))}}exports.RNN=k,k.className=\"RNN\",t.serialization.registerClass(k);class m extends n.Layer{}exports.RNNCell=m;class f extends m{constructor(t){super(t),this.DEFAULT_ACTIVATION=\"tanh\",this.DEFAULT_KERNEL_INITIALIZER=\"glorotNormal\",this.DEFAULT_RECURRENT_INITIALIZER=\"orthogonal\",this.DEFAULT_BIAS_INITIALIZER=\"zeros\",this.units=t.units,(0,o.assertPositiveInteger)(this.units,\"units\"),this.activation=(0,e.getActivation)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,l.getInitializer)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,l.getInitializer)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,l.getInitializer)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,u.getRegularizer)(t.kernelRegularizer),this.recurrentRegularizer=(0,u.getRegularizer)(t.recurrentRegularizer),this.biasRegularizer=(0,u.getRegularizer)(t.biasRegularizer),this.kernelConstraint=(0,s.getConstraint)(t.kernelConstraint),this.recurrentConstraint=(0,s.getConstraint)(t.recurrentConstraint),this.biasConstraint=(0,s.getConstraint)(t.biasConstraint),this.dropout=h.min([1,h.max([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=h.min([1,h.max([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,c.getExactlyOneShape)(t),this.kernel=this.addWeight(\"kernel\",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight(\"recurrent_kernel\",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight(\"bias\",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(e,r){return(0,t.tidy)(()=>{if(2!==(e=e).length)throw new a.ValueError(`SimpleRNNCell expects 2 input Tensors, got ${e.length}.`);let s=e[1];e=e[0];const n=null!=r.training&&r.training;let l;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=E({ones:()=>t.onesLike(e),rate:this.dropout,training:n})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=E({ones:()=>t.onesLike(s),rate:this.recurrentDropout,training:n}));const u=this.dropoutMask,o=this.recurrentDropoutMask;l=null!=u?i.dot(t.mul(e,u),this.kernel.read()):i.dot(e,this.kernel.read()),null!=this.bias&&(l=i.biasAdd(l,this.bias.read())),null!=o&&(s=t.mul(s,o));let h=t.add(l,i.dot(s,this.recurrentKernel.read()));return null!=this.activation&&(h=this.activation.apply(h)),[h,h]})}getConfig(){const t=super.getConfig(),i={units:this.units,activation:(0,e.serializeActivation)(this.activation),useBias:this.useBias,kernelInitializer:(0,l.serializeInitializer)(this.kernelInitializer),recurrentInitializer:(0,l.serializeInitializer)(this.recurrentInitializer),biasInitializer:(0,l.serializeInitializer)(this.biasInitializer),kernelRegularizer:(0,u.serializeRegularizer)(this.kernelRegularizer),recurrentRegularizer:(0,u.serializeRegularizer)(this.recurrentRegularizer),biasRegularizer:(0,u.serializeRegularizer)(this.biasRegularizer),activityRegularizer:(0,u.serializeRegularizer)(this.activityRegularizer),kernelConstraint:(0,s.serializeConstraint)(this.kernelConstraint),recurrentConstraint:(0,s.serializeConstraint)(this.recurrentConstraint),biasConstraint:(0,s.serializeConstraint)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,i)}}exports.SimpleRNNCell=f,f.className=\"SimpleRNNCell\",t.serialization.registerClass(f);class A extends k{constructor(t){t.cell=new f(t),super(t)}call(e,i){return(0,t.tidy)(()=>{null!=this.cell.dropoutMask&&(t.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(t.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const r=null==i?null:i.mask,s=null==i?null:i.training,n=null==i?null:i.initialState;return super.call(e,{mask:r,training:s,initialState:n})})}static fromConfig(t,e){return new t(e)}}exports.SimpleRNN=A,A.className=\"SimpleRNN\",t.serialization.registerClass(A);class S extends m{constructor(t){if(super(t),this.DEFAULT_ACTIVATION=\"tanh\",this.DEFAULT_RECURRENT_ACTIVATION=\"hardSigmoid\",this.DEFAULT_KERNEL_INITIALIZER=\"glorotNormal\",this.DEFAULT_RECURRENT_INITIALIZER=\"orthogonal\",this.DEFAULT_BIAS_INITIALIZER=\"zeros\",t.resetAfter)throw new a.ValueError(\"GRUCell does not support reset_after parameter set to true.\");this.units=t.units,(0,o.assertPositiveInteger)(this.units,\"units\"),this.activation=(0,e.getActivation)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,e.getActivation)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,l.getInitializer)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,l.getInitializer)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,l.getInitializer)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=(0,u.getRegularizer)(t.kernelRegularizer),this.recurrentRegularizer=(0,u.getRegularizer)(t.recurrentRegularizer),this.biasRegularizer=(0,u.getRegularizer)(t.biasRegularizer),this.kernelConstraint=(0,s.getConstraint)(t.kernelConstraint),this.recurrentConstraint=(0,s.getConstraint)(t.recurrentConstraint),this.biasConstraint=(0,s.getConstraint)(t.biasConstraint),this.dropout=h.min([1,h.max([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=h.min([1,h.max([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=(0,c.getExactlyOneShape)(t))[t.length-1];this.kernel=this.addWeight(\"kernel\",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight(\"recurrent_kernel\",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight(\"bias\",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(e,r){return(0,t.tidy)(()=>{if(2!==(e=e).length)throw new a.ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \"+`${e.length}.`);const s=null!=r.training&&r.training;let n=e[1];e=e[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=E({ones:()=>t.onesLike(e),rate:this.dropout,training:s,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=E({ones:()=>t.onesLike(n),rate:this.recurrentDropout,training:s,count:3}));const l=this.dropoutMask,u=this.recurrentDropoutMask;let o,h,c;0<this.dropout&&this.dropout<1&&(e=t.mul(e,l[0]));let p=i.dot(e,this.kernel.read());this.useBias&&(p=i.biasAdd(p,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(n=t.mul(n,u[0]));const g=this.recurrentKernel.read(),[z,d]=t.split(g,[2*this.units,this.units],g.rank-1),I=i.dot(n,z),[R,k,m]=t.split(p,3,p.rank-1),[f,A]=t.split(I,2,I.rank-1);o=this.recurrentActivation.apply(t.add(R,f)),h=this.recurrentActivation.apply(t.add(k,A));const S=i.dot(t.mul(h,n),d);c=this.activation.apply(t.add(m,S));const C=t.add(t.mul(o,n),t.mul(t.add(1,t.neg(o)),c));return[C,C]})}getConfig(){const t=super.getConfig(),i={units:this.units,activation:(0,e.serializeActivation)(this.activation),recurrentActivation:(0,e.serializeActivation)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,l.serializeInitializer)(this.kernelInitializer),recurrentInitializer:(0,l.serializeInitializer)(this.recurrentInitializer),biasInitializer:(0,l.serializeInitializer)(this.biasInitializer),kernelRegularizer:(0,u.serializeRegularizer)(this.kernelRegularizer),recurrentRegularizer:(0,u.serializeRegularizer)(this.recurrentRegularizer),biasRegularizer:(0,u.serializeRegularizer)(this.biasRegularizer),activityRegularizer:(0,u.serializeRegularizer)(this.activityRegularizer),kernelConstraint:(0,s.serializeConstraint)(this.kernelConstraint),recurrentConstraint:(0,s.serializeConstraint)(this.recurrentConstraint),biasConstraint:(0,s.serializeConstraint)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,i)}}exports.GRUCell=S,S.className=\"GRUCell\",t.serialization.registerClass(S);class C extends k{constructor(t){0===t.implementation&&console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"),t.cell=new S(t),super(t)}call(e,i){return(0,t.tidy)(()=>{null!=this.cell.dropoutMask&&(t.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(t.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const r=null==i?null:i.mask,s=null==i?null:i.training,n=null==i?null:i.initialState;return super.call(e,{mask:r,training:s,initialState:n})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}exports.GRU=C,C.className=\"GRU\",t.serialization.registerClass(C);class b extends m{constructor(t){super(t),this.DEFAULT_ACTIVATION=\"tanh\",this.DEFAULT_RECURRENT_ACTIVATION=\"hardSigmoid\",this.DEFAULT_KERNEL_INITIALIZER=\"glorotNormal\",this.DEFAULT_RECURRENT_INITIALIZER=\"orthogonal\",this.DEFAULT_BIAS_INITIALIZER=\"zeros\",this.units=t.units,(0,o.assertPositiveInteger)(this.units,\"units\"),this.activation=(0,e.getActivation)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,e.getActivation)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,l.getInitializer)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,l.getInitializer)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,l.getInitializer)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=(0,u.getRegularizer)(t.kernelRegularizer),this.recurrentRegularizer=(0,u.getRegularizer)(t.recurrentRegularizer),this.biasRegularizer=(0,u.getRegularizer)(t.biasRegularizer),this.kernelConstraint=(0,s.getConstraint)(t.kernelConstraint),this.recurrentConstraint=(0,s.getConstraint)(t.recurrentConstraint),this.biasConstraint=(0,s.getConstraint)(t.biasConstraint),this.dropout=h.min([1,h.max([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=h.min([1,h.max([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const r=(t=(0,c.getExactlyOneShape)(t))[t.length-1];let s;if(this.kernel=this.addWeight(\"kernel\",[r,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight(\"recurrent_kernel\",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,r=this.units;s=new((e=class extends l.Initializer{apply(e,s){const n=t.apply([r]),a=(new l.Ones).apply([r]),u=t.apply([2*r]);return i.concatAlongFirstAxis(i.concatAlongFirstAxis(n,a),u)}}).className=\"CustomInit\",e)}else s=this.biasInitializer;this.bias=this.addWeight(\"bias\",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(e,r){return(0,t.tidy)(()=>{const s=null!=r.training&&r.training;if(3!==(e=e).length)throw new a.ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \"+`${e.length}.`);let n=e[1];const l=e[2];e=e[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=E({ones:()=>t.onesLike(e),rate:this.dropout,training:s,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=E({ones:()=>t.onesLike(n),rate:this.recurrentDropout,training:s,count:4}));const u=this.dropoutMask,o=this.recurrentDropoutMask;let h,c,p,g;0<this.dropout&&this.dropout<1&&(e=t.mul(e,u[0]));let z=i.dot(e,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(n=t.mul(n,o[0])),z=t.add(z,i.dot(n,this.recurrentKernel.read())),this.useBias&&(z=i.biasAdd(z,this.bias.read()));const[d,I,R,k]=t.split(z,4,z.rank-1);h=this.recurrentActivation.apply(d),c=this.recurrentActivation.apply(I),p=t.add(t.mul(c,l),t.mul(h,this.activation.apply(R))),g=this.recurrentActivation.apply(k);const m=t.mul(g,this.activation.apply(p));return[m,m,p]})}getConfig(){const t=super.getConfig(),i={units:this.units,activation:(0,e.serializeActivation)(this.activation),recurrentActivation:(0,e.serializeActivation)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,l.serializeInitializer)(this.kernelInitializer),recurrentInitializer:(0,l.serializeInitializer)(this.recurrentInitializer),biasInitializer:(0,l.serializeInitializer)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:(0,u.serializeRegularizer)(this.kernelRegularizer),recurrentRegularizer:(0,u.serializeRegularizer)(this.recurrentRegularizer),biasRegularizer:(0,u.serializeRegularizer)(this.biasRegularizer),activityRegularizer:(0,u.serializeRegularizer)(this.activityRegularizer),kernelConstraint:(0,s.serializeConstraint)(this.kernelConstraint),recurrentConstraint:(0,s.serializeConstraint)(this.recurrentConstraint),biasConstraint:(0,s.serializeConstraint)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,i)}}exports.LSTMCell=b,b.className=\"LSTMCell\",t.serialization.registerClass(b);class y extends k{constructor(t){0===t.implementation&&console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"),t.cell=new b(t),super(t)}call(e,i){return(0,t.tidy)(()=>{null!=this.cell.dropoutMask&&(t.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(t.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const r=null==i?null:i.mask,s=null==i?null:i.training,n=null==i?null:i.initialState;return super.call(e,{mask:r,training:s,initialState:n})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}exports.LSTM=y,y.className=\"LSTM\",t.serialization.registerClass(y);class N extends m{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(e,i){return(0,t.tidy)(()=>{let t=(e=e).slice(1);const r=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?r.push(t.splice(0,e.stateSize.length)):r.push(t.splice(0,1));r.reverse();const s=[];let n;for(let a=0;a<this.cells.length;++a){const l=this.cells[a];t=r[a],n=0===a?[e[0]].concat(t):[n[0]].concat(t),n=l.call(n,i),s.push(n.slice(1))}t=[];for(const e of s.slice().reverse())t.push(...e);return[n[0]].concat(t)})}build(t){let e;(0,c.isArrayOfShapes)(t)&&(t=t[0]),t=t,this.cells.forEach((i,s)=>{(0,r.nameScope)(`RNNCell_${s}`,()=>{i.build(t),e=Array.isArray(i.stateSize)?i.stateSize[0]:i.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map(t=>({className:t.getClassName(),config:t.getConfig()}))};return Object.assign({},t,e)}static fromConfig(t,e,i={}){const r=[];for(const s of e.cells)r.push((0,g.deserialize)(s,i));return new t({cells:r})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return(0,p.batchGetValue)(t)}setWeights(t){const e=[];for(const i of this.cells){const r=i.weights.length,s=t.splice(r);for(let t=0;t<i.weights.length;++t)e.push([i.weights[t],s[t]])}(0,p.batchSetValue)(e)}}function E(e){const{ones:r,rate:s,training:n=!1,count:a=1}=e,l=()=>i.dropout(r(),s),u=()=>i.inTrainPhase(l,r,n);return!a||a<=1?t.keep(u().clone()):Array(a).fill(void 0).map(u).map(e=>t.keep(e.clone()))}exports.StackedRNNCells=N,N.className=\"StackedRNNCells\",t.serialization.registerClass(N);"},"sourceMaps":null,"error":null,"hash":"936943903f881a8b4ff6ebf934611c52","cacheData":{"env":{}}}